import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import torch
import os
from transformers import logging
logging.set_verbosity_info()

# ========================
# CONFIG
# ========================
# Modelos utilizados:
# - EMBED_MODEL: para convertir preguntas y documentos en vectores num√©ricos (embeddings).
# - QA_MODEL: modelo extractivo para responder preguntas en espa√±ol a partir de un contexto.
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
QA_MODEL = "mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"  # modelo extractivo en espa√±ol

# ========================
# CARGA DE EMBEDDINGS
# ========================
print("üîç Cargando modelo de embeddings...")
embedding_model = SentenceTransformer(
    EMBED_MODEL,
    device='cuda' if torch.cuda.is_available() else 'cpu' # Usa GPU si est√° disponible
)
print("‚úÖ Embeddings cargados")

# ========================
# CARGA DEL MODELO QA
# ========================
print(f"ü§ñ Cargando modelo de QA: {QA_MODEL}...")
qa_pipeline = pipeline(
    "question-answering", # Pipeline de preguntas y respuestas (extractivo)
    model=QA_MODEL,
    tokenizer=QA_MODEL
)
print("‚úÖ Pipeline QA listo")

# ========================
# CARGA DEL √çNDICE RAG
# ========================
# Lee el √≠ndice FAISS generado previamente y los metadatos (texto chunkificado)
print("üìö Cargando √≠ndice RAG...")
index = faiss.read_index("rag_index.faiss")
with open("rag_metadata.json", "r", encoding="utf-8") as f:
    metadatos_json = json.load(f)
chunks = metadatos_json.get("chunks", [])
print(f"‚úÖ √çndice y metadatos cargados ({len(chunks)} chunks)")

# ========================
# FUNCIONES RAG
# ========================
def recuperar_contexto(pregunta, k=3):
    """Recupera los k chunks m√°s relevantes usando embeddings + FAISS"""
    # Convierte la pregunta en vector de embeddings
    pregunta_emb = embedding_model.encode(pregunta)
     # Busca los k m√°s cercanos en el √≠ndice FAISS
    _, indices = index.search(np.array([pregunta_emb]), k)
    # Recupera el texto original de cada √≠ndice encontrado
    contextos = [chunks[i] for i in indices[0] if 0 <= i < len(chunks)]
    return "\n\n".join(contextos)

def responder_pregunta(pregunta):
    """
    Usa el pipeline de QA (extractivo) para responder
    """
    contexto = recuperar_contexto(pregunta)
    if not contexto.strip():
        return "‚ö†Ô∏è No encontr√© contexto relevante en los documentos."
    # Pasa la pregunta y el contexto al modelo de QA extractivo
    resultado = qa_pipeline({
        "question": pregunta,
        "context": contexto
    })

    return resultado["answer"]
