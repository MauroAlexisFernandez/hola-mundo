import os
import json
import faiss
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
from PyPDF2 import PdfReader  

# ============================
# CONFIG
# ============================
# Rutas y par√°metros principales
PDF_PATH = "TFM_Mauro_Alexis_Fernandez.pdf" # PDF origen para construir el √≠ndice
CHUNK_SIZE = 700 # Cantidad de caracteres por fragmento (chunk)
CHUNK_OVERLAP = 100 # Superposici√≥n entre fragmentos para no perder contexto
INDEX_NAME = "rag_index.faiss" # Nombre del archivo donde se guardar√° el √≠ndice FAISS
METADATA_NAME = "rag_metadata.json"  # Archivo para guardar texto de cada chunk
MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" # Modelo de embeddings

# ============================
# FUNCIONES
# ============================
def smart_chunk(text, chunk_size=700, overlap=100):
    """Divide un texto largo en chunks con solapamiento"""
    if len(text) <= chunk_size:
        return [text]
    chunks = []
    start = 0
    while start < len(text):
        # Genera el fragmento desde start hasta start+chunk_size
        end = min(len(text), start + chunk_size)
        chunks.append(text[start:end])
        # Avanza al siguiente fragmento restando el solapamiento
        start += chunk_size - overlap
    return chunks

def extract_chunks_from_pdf(path):
    reader = PdfReader(path)
    full_text = ""
    for page in reader.pages:
         # Concatena texto de cada p√°gina
        full_text += page.extract_text() + "\n"
    return smart_chunk(full_text)

# ============================
# MAIN
# ============================
print("‚úÖ Cargando modelo de embeddings...")
model = SentenceTransformer(MODEL_NAME) # Carga el modelo de transformadores para embeddings

print("üìÑ Extrayendo y dividiendo texto del PDF...")
chunks = extract_chunks_from_pdf(PDF_PATH) # Lee y chunkifica el PDF
print(f"‚úÇÔ∏è Total de chunks generados desde el PDF: {len(chunks)}")

print("üî¢ Generando embeddings...")
embeddings = model.encode(chunks, show_progress_bar=True) # Convierte cada chunk a un vector


print("üíæ Guardando √≠ndice FAISS y metadatos...")
d = embeddings.shape[1]  # Dimensi√≥n de los vectores
index = faiss.IndexFlatL2(d) # Crea √≠ndice plano (L2)
index.add(np.array(embeddings))  # Agrega todos los vectores al √≠ndice
faiss.write_index(index, INDEX_NAME) # Guarda √≠ndice en disco

# Guarda tambi√©n el texto original de cada chunk para referencia
with open(METADATA_NAME, "w", encoding="utf-8") as f:
    json.dump({"chunks": chunks}, f, ensure_ascii=False, indent=2)

print("‚úÖ ¬°√çndice RAG creado con √©xito desde el PDF!")
